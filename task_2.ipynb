{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Визначаємо базову веб сторінку з якою будемо працювати\n",
    "base_url = \"http://quotes.toscrape.com\"\n",
    "\n",
    "# Запитуємо сторінку, перевіряємо статус відповіді, та передаємо на обробку BeautifulSoup  \n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() \n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "# Функція по пошуку та отримання даних\n",
    "def scrape_quotes():\n",
    "    quotes = []\n",
    "    authors_info = []\n",
    "    url = base_url\n",
    "    while url:\n",
    "        try:\n",
    "            # на базовій сторінці шукаємо всі цитати та відокремлюємо їх дані - Тект, автор, теги\n",
    "            soup = get_soup(url)\n",
    "            quote_divs = soup.find_all('div', class_='quote')\n",
    "            for quote_div in quote_divs:\n",
    "                text = quote_div.find('span', class_='text').text\n",
    "                author = quote_div.find('small', class_='author').text\n",
    "                tags = [tag.text for tag in quote_div.find_all('a', class_='tag')]\n",
    "                \n",
    "                # Записуємо все у список словників\n",
    "                quotes.append({\n",
    "                    'quote': text,\n",
    "                    'author': author,\n",
    "                    'tags': tags\n",
    "                })\n",
    "                \n",
    "                # до базової сторінки додаємо посилання на сторінку автора та далі збираємо з неї інформацю\n",
    "                author_url = base_url + quote_div.find('a')['href']\n",
    "                if not any(author_info['fullname'] == author for author_info in authors_info): # перевірка наявності автора у authors_info\n",
    "                    author_soup = get_soup(author_url)\n",
    "                    born_date = author_soup.find('span', class_='author-born-date').text\n",
    "                    born_location = author_soup.find('span', class_='author-born-location').text\n",
    "                    description = author_soup.find('div', class_='author-description').text.strip()\n",
    "                    \n",
    "                    # Записуємо все у список словників\n",
    "                    authors_info.append({\n",
    "                        'fullname': author,\n",
    "                        'born_date': born_date,\n",
    "                        'born_location': born_location,\n",
    "                        'description': description\n",
    "                    })\n",
    "            \n",
    "            # Додаємо перехід на іншу сторінку\n",
    "            next_btn = soup.find('li', class_='next')\n",
    "            \n",
    "            # Якщо наступної сторінки нема повертаємо None для зупинки циклу\n",
    "            url = base_url + next_btn.find('a')['href'] if next_btn else None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return quotes, authors_info\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quotes, authors_info = scrape_quotes()\n",
    "    save_to_json(quotes, 'quotes.json')\n",
    "    save_to_json(authors_info, 'authors.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def import_to_mongodb():\n",
    "    client = MongoClient(\"mongodb+srv://koshelevskiyv:GOIT2024_Zp@goitlearn.1laswll.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client.get_database(\"quotes_db\")\n",
    "    \n",
    "    quotes_collection = db.quotes\n",
    "    authors_collection = db.authors\n",
    "    \n",
    "    quotes = load_json('quotes.json')\n",
    "    authors = load_json('authors.json')\n",
    "    \n",
    "    try:\n",
    "        quotes_collection.insert_many(quotes)\n",
    "        authors_collection.insert_many(authors)\n",
    "        print(\"Data imported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import_to_mongodb()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
